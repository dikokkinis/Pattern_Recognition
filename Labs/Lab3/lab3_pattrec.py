{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown]\n# # 3ο Εργαστήριο Αναγνώριση Προτύπων\n# \n# ❐ Δημήτριος Κοκκίνης: 03118896\n# \n# ❐ Χριστίνα Ρεντίφη: 03118217\n\n# %% [markdown]\n# Αρχικά κάνουμε import τις απαραίτητες βιβλιοθήκες και εισάγουμε τα δεδομένα στο kaggle\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:26:44.786495Z\",\"iopub.execute_input\":\"2023-01-08T18:26:44.786761Z\",\"iopub.status.idle\":\"2023-01-08T18:26:48.437578Z\",\"shell.execute_reply.started\":\"2023-01-08T18:26:44.786733Z\",\"shell.execute_reply\":\"2023-01-08T18:26:48.436775Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport copy\n\nimport matplotlib.mlab as mlab\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import SubsetRandomSampler, DataLoader\n\nimport re\n\nimport joblib\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:26:53.629312Z\",\"iopub.execute_input\":\"2023-01-08T18:26:53.630153Z\",\"iopub.status.idle\":\"2023-01-08T18:26:58.465321Z\",\"shell.execute_reply.started\":\"2023-01-08T18:26:53.630115Z\",\"shell.execute_reply\":\"2023-01-08T18:26:58.464523Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/patreco3-multitask-affective-music/data/'):\n    for filename in filenames[:1]:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [markdown]\n# ## ΒΗΜΑ 1\n# \n# ### Α)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:27:10.901295Z\",\"iopub.execute_input\":\"2023-01-08T18:27:10.901570Z\",\"iopub.status.idle\":\"2023-01-08T18:27:10.937309Z\",\"shell.execute_reply.started\":\"2023-01-08T18:27:10.901541Z\",\"shell.execute_reply\":\"2023-01-08T18:27:10.936578Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nspec1 = np.load('/kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/train/100839.fused.full.npy')\n\nspec2 = np.load('/kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/train/116543.fused.full.npy')\n\n# %% [markdown]\n# This is a fused spectrogram + chromagram. The shape is (mel + chroma frequencies, timesteps)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:27:13.418353Z\",\"iopub.execute_input\":\"2023-01-08T18:27:13.418620Z\",\"iopub.status.idle\":\"2023-01-08T18:27:13.424235Z\",\"shell.execute_reply.started\":\"2023-01-08T18:27:13.418591Z\",\"shell.execute_reply\":\"2023-01-08T18:27:13.423423Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nprint(spec1.shape)  # (frequencies x time steps)\n\nprint(spec2.shape)\n\n# %% [markdown]\n# ### B)\n# \n# To decompose into the mel spectrogram and chromagram you can run:\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:27:22.054040Z\",\"iopub.execute_input\":\"2023-01-08T18:27:22.054338Z\",\"iopub.status.idle\":\"2023-01-08T18:27:22.058314Z\",\"shell.execute_reply.started\":\"2023-01-08T18:27:22.054294Z\",\"shell.execute_reply\":\"2023-01-08T18:27:22.057461Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nmel1, chroma1 = spec1[:128], spec1[128:]\n\nmel2, chroma2 = spec2[:128], spec2[128:]\n\n# %% [markdown]\n# ### Γ)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:27:28.367763Z\",\"iopub.execute_input\":\"2023-01-08T18:27:28.368049Z\",\"iopub.status.idle\":\"2023-01-08T18:27:29.253440Z\",\"shell.execute_reply.started\":\"2023-01-08T18:27:28.368019Z\",\"shell.execute_reply\":\"2023-01-08T18:27:29.252838Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# Plot the spectrograms\n\nfig1, ax1 = plt.subplots()\nimg1 = librosa.display.specshow(mel1, x_axis='time', y_axis='linear', ax=ax1)\nax1.set(title='Spectrogram 1')\nfig1.colorbar(img1, ax=ax1, format=\"%+2.f dB\")\n\nfig2, ax2 = plt.subplots()\nimg2 = librosa.display.specshow(mel2, x_axis='time', y_axis='linear', ax=ax2)\nax2.set(title='Spectrogram 2')\nfig2.colorbar(img2, ax=ax2, format=\"%+2.f dB\")\n\n# %% [markdown]\n# Τα φασματογραφήματα που απεικονίζονται παραπάνω μας δείχνουν την φασματική πυκνότητα του ήχου. \n# Βλέπουμε δύο διαστάσεις όπως φαίνεται και παραπάνω και η τρίτη διάσταση \n# παρουσιάζεται με την μορφή χρώματος. Στον οριζόντιο άξονα έχουμε την διάσταση του χρόνου (αυξάνεται από \n# αριστερά προς τα δεξιά). Ο κατακόρυφος άξονας αντιστοιχεί στην συχνότητα \n# και μπορεί να θεωρηθεί ως το pitch (τόνος) με τις χαμηλότερες \n# συχνότητες να ξεκινάνε από κάτω και να αυξάνονται καθώς ανεβαίνουμε \n# κατακόρυφα. Το χρώμα που αντιπροσωπεύει την τρίτη διάσταση μας δείχνει το \n# πλάτος (ή ενέργεια ή διαφορετικά ένταση ”loudness”) μιας δεδομένης \n# συχνότητας σε μια δεδομένη χρονική στιγμή. Τα πιο σκούρα χρώματα \n# αντιπροσωπεύουν χαμηλότερα πλάτη και τα πιο ανοιχτά πιο \n# υψηλά πλάτη.\n\n# %% [markdown]\n# ## BHMA 2\n# \n# ### A)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:27:37.943503Z\",\"iopub.execute_input\":\"2023-01-08T18:27:37.943802Z\",\"iopub.status.idle\":\"2023-01-08T18:27:37.949045Z\",\"shell.execute_reply.started\":\"2023-01-08T18:27:37.943757Z\",\"shell.execute_reply\":\"2023-01-08T18:27:37.948310Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nprint(mel1.shape, mel2.shape)\nprint()\nprint(\"Timesteps of spectogram 1 are: \" + str(mel1.shape[1]))\nprint(\"Timesteps of spectogram 2 are: \" + str(mel2.shape[1]))\n\n# %% [markdown]\n# Έχουμε πολλά χρονικά βήματα οπότε πρέπει κάπως να τα μειώσουμε ώστε να μπορέσουμε να εκπαιδέυσουμε ένα LSTM\n\n# %% [markdown]\n# ### Β)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:27:55.139447Z\",\"iopub.execute_input\":\"2023-01-08T18:27:55.139712Z\",\"iopub.status.idle\":\"2023-01-08T18:27:55.157050Z\",\"shell.execute_reply.started\":\"2023-01-08T18:27:55.139684Z\",\"shell.execute_reply\":\"2023-01-08T18:27:55.156358Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nspec1 = np.load('/kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/train/100839.fused.full.npy')\n\nspec2 = np.load('/kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/train/116543.fused.full.npy')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:27:58.025891Z\",\"iopub.execute_input\":\"2023-01-08T18:27:58.026168Z\",\"iopub.status.idle\":\"2023-01-08T18:27:58.031422Z\",\"shell.execute_reply.started\":\"2023-01-08T18:27:58.026141Z\",\"shell.execute_reply\":\"2023-01-08T18:27:58.030401Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nprint(spec1.shape)\nprint(spec2.shape)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:28:04.240525Z\",\"iopub.execute_input\":\"2023-01-08T18:28:04.240811Z\",\"iopub.status.idle\":\"2023-01-08T18:28:04.244954Z\",\"shell.execute_reply.started\":\"2023-01-08T18:28:04.240762Z\",\"shell.execute_reply\":\"2023-01-08T18:28:04.244205Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# mel spectrogram + chromagram, timesteps\nmel1_beat, chroma1_beat = spec1[:128], spec1[128:]\nmel2_beat, chroma2_beat = spec2[:128], spec2[128:]\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:28:06.654913Z\",\"iopub.execute_input\":\"2023-01-08T18:28:06.655684Z\",\"iopub.status.idle\":\"2023-01-08T18:28:07.260273Z\",\"shell.execute_reply.started\":\"2023-01-08T18:28:06.655644Z\",\"shell.execute_reply\":\"2023-01-08T18:28:07.259603Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# Plot the spectrograms\n\nfig1, ax1 = plt.subplots()\nimg1 = librosa.display.specshow(mel1_beat, x_axis='time', y_axis='linear', ax=ax1)\nax1.set(title='Spectrogram 1')\nfig1.colorbar(img1, ax=ax1, format=\"%+2.f dB\")\n\nfig2, ax2 = plt.subplots()\nimg2 = librosa.display.specshow(mel2_beat, x_axis='time', y_axis='linear', ax=ax2)\nax2.set(title='Spectrogram 2')\nfig2.colorbar(img2, ax=ax2, format=\"%+2.f dB\")\n\n# %% [markdown]\n# Παρατηρούμε τώρα ότι τα γραφήματα δεν είναι τόσο smooth, δεν έχουν τόσο μεγάλη ακρίβεια καθώς έχουν μειωθεί οι διαστάσεις με αποτέλεσμα να φαίνονται πιο \"pixelated\". Ωστόσο η πληροφορία δεν χάνεται σημαντικά επομένως μπορούμε να εκπαιδεύσουμε το LSTM πολύ πιο αποδοτικά.\n\n# %% [markdown]\n# ## ΒΗΜΑ 3\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:28:16.417661Z\",\"iopub.execute_input\":\"2023-01-08T18:28:16.418389Z\",\"iopub.status.idle\":\"2023-01-08T18:28:16.423164Z\",\"shell.execute_reply.started\":\"2023-01-08T18:28:16.418353Z\",\"shell.execute_reply\":\"2023-01-08T18:28:16.422378Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# Chromograms, 12 musical notes\n# timesteps --> 2nd dimension\nprint(chroma1.shape) \nprint(chroma2.shape)\n\nprint(chroma1_beat.shape)\nprint(chroma2_beat.shape)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:28:26.722893Z\",\"iopub.execute_input\":\"2023-01-08T18:28:26.723427Z\",\"iopub.status.idle\":\"2023-01-08T18:28:27.342652Z\",\"shell.execute_reply.started\":\"2023-01-08T18:28:26.723391Z\",\"shell.execute_reply\":\"2023-01-08T18:28:27.341938Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nfig1, ax1 = plt.subplots()\nimg1 = librosa.display.specshow(chroma1, x_axis='time', y_axis='linear', ax=ax1)\nax1.set(title='Chromagram 1')\nfig1.colorbar(img1, ax=ax1, format=\"%+2.f dB\")\n\nfig2, ax2 = plt.subplots()\nimg2 = librosa.display.specshow(chroma2, x_axis='time', y_axis='linear', ax=ax2)\nax2.set(title='Chromagram 2')\nfig2.colorbar(img2, ax=ax2, format=\"%+2.f dB\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:28:53.391165Z\",\"iopub.execute_input\":\"2023-01-08T18:28:53.391459Z\",\"iopub.status.idle\":\"2023-01-08T18:28:53.962149Z\",\"shell.execute_reply.started\":\"2023-01-08T18:28:53.391427Z\",\"shell.execute_reply\":\"2023-01-08T18:28:53.961472Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n#Chromagrams after dimensionality reduction\n\nfig1, ax1 = plt.subplots()\nimg1 = librosa.display.specshow(chroma1_beat, x_axis='time', y_axis='linear', ax=ax1)\nax1.set(title='Spectrogram 1')\nfig1.colorbar(img1, ax=ax1, format=\"%+2.f dB\")\n\nfig2, ax2 = plt.subplots()\nimg2 = librosa.display.specshow(chroma2_beat, x_axis='time', y_axis='linear', ax=ax2)\nax2.set(title='Spectrogram 2')\nfig2.colorbar(img2, ax=ax2, format=\"%+2.f dB\")\n\n# %% [markdown]\n# ## ΒΗΜΑ 4\n\n# %% [markdown]\n# ## Creating a Pytorch Dataset\n\n# %% [markdown]\n# We provide helper code that creates a pytorch dataset using the competition data:\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:29:02.215219Z\",\"iopub.execute_input\":\"2023-01-08T18:29:02.215496Z\",\"iopub.status.idle\":\"2023-01-08T18:30:22.769558Z\",\"shell.execute_reply.started\":\"2023-01-08T18:29:02.215466Z\",\"shell.execute_reply\":\"2023-01-08T18:30:22.767963Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# Combine similar classes and remove underrepresented classes\nclass_mapping = {\n    'Rock': 'Rock',\n    'Psych-Rock': 'Rock',\n    'Indie-Rock': None,\n    'Post-Rock': 'Rock',\n    'Psych-Folk': 'Folk',\n    'Folk': 'Folk',\n    'Metal': 'Metal',\n    'Punk': 'Metal',\n    'Post-Punk': None,\n    'Trip-Hop': 'Trip-Hop',\n    'Pop': 'Pop',\n    'Electronic': 'Electronic',\n    'Hip-Hop': 'Hip-Hop',\n    'Classical': 'Classical',\n    'Blues': 'Blues',\n    'Chiptune': 'Electronic',\n    'Jazz': 'Jazz',\n    'Soundtrack': None,\n    'International': None,\n    'Old-Time': None\n}\n\ndef torch_train_val_split(\n        dataset, batch_train, batch_eval,\n        val_size=.2, shuffle=True, seed=None):\n    # Creating data indices for training and validation splits:\n    \n    dataset_size = len(dataset) #length of dataset\n    indices = list(range(dataset_size)) #indices for each element in dataset [0,1,2,..., length of dataset-1]\n    val_split = int(np.floor(val_size * dataset_size)) #στρογγυλοποίηση του val_size*dataset_size\n    if shuffle:\n        np.random.seed(seed)\n        np.random.shuffle(indices)\n    # χωρισμός των δεικτών που θα είναι για το train set και το validation set\n    train_indices = indices[val_split:] \n    val_indices = indices[:val_split]\n\n    # Creating PT data samplers and loaders:\n    train_sampler = SubsetRandomSampler(train_indices)\n    val_sampler = SubsetRandomSampler(val_indices)\n\n    train_loader = DataLoader(dataset,\n                              batch_size=batch_train,\n                              sampler=train_sampler)\n    val_loader = DataLoader(dataset,\n                            batch_size=batch_eval,\n                            sampler=val_sampler)\n    return train_loader, val_loader\n\n\n\n# Helper functions to read fused, mel, and chromagram\ndef read_fused_spectrogram(spectrogram_file):\n    spectrogram = np.load(spectrogram_file)\n    return spectrogram.T\n\n\ndef read_mel_spectrogram(spectrogram_file):\n    spectrogram = np.load(spectrogram_file)[:128]\n    return spectrogram.T\n\n    \ndef read_chromagram(spectrogram_file):\n    spectrogram = np.load(spectrogram_file)[128:]\n    return spectrogram.T\n\n\nclass LabelTransformer(LabelEncoder):\n    def inverse(self, y):\n        try:\n            return super(LabelTransformer, self).inverse_transform(y)\n        except:\n            return super(LabelTransformer, self).inverse_transform([y])\n\n    def transform(self, y):\n        try:\n            return super(LabelTransformer, self).transform(y)\n        except:\n            return super(LabelTransformer, self).transform([y])\n        \n        \nclass PaddingTransform(object):\n    def __init__(self, max_length, padding_value=0):\n        self.max_length = max_length\n        self.padding_value = padding_value\n\n    def __call__(self, s):\n        if len(s) == self.max_length:\n            return s\n\n        if len(s) > self.max_length:\n            return s[:self.max_length]\n\n        if len(s) < self.max_length:\n            s1 = copy.deepcopy(s)\n            pad = np.zeros((self.max_length - s.shape[0], s.shape[1]), dtype=np.float32)\n            s1 = np.vstack((s1, pad))\n            return s1\n        \n        \n# Pytorch Dataset Class for creating the dataset\nclass SpectrogramDataset(Dataset):\n    def __init__(self, path, class_mapping=None, train=True, max_length=-1, read_spec_fn=read_fused_spectrogram):\n        t = 'train' if train else 'test'\n        p = os.path.join(path, t)\n        self.index = os.path.join(path, \"{}_labels.txt\".format(t))\n        self.files, labels = self.get_files_labels(self.index, class_mapping)\n        self.feats = [read_spec_fn(os.path.join(p, f)) for f in self.files]\n        self.feat_dim = self.feats[0].shape[1]\n        self.lengths = [len(i) for i in self.feats]\n        self.max_length = max(self.lengths) if max_length <= 0 else max_length\n        self.zero_pad_and_stack = PaddingTransform(self.max_length)\n        self.label_transformer = LabelTransformer()\n        if isinstance(labels, (list, tuple)):\n            self.labels = np.array(self.label_transformer.fit_transform(labels)).astype('int64')\n\n    def get_files_labels(self, txt, class_mapping):\n        with open(txt, 'r') as fd:\n            lines = [l.rstrip().split('\\t') for l in fd.readlines()[1:]]\n        files, labels = [], []\n        for l in lines:\n            label = l[1]\n            if class_mapping:\n                label = class_mapping[l[1]]\n            if not label:\n                continue\n            # Kaggle automatically unzips the npy.gz format so this hack is needed\n            _id = l[0].split('.')[0]\n            npy_file = '{}.fused.full.npy'.format(_id)\n            files.append(npy_file)\n            labels.append(label)\n        return files, labels\n\n    def __getitem__(self, item):\n        # TODO: Inspect output and comment on how the output is formatted\n        l = min(self.lengths[item], self.max_length)\n        return self.zero_pad_and_stack(self.feats[item]), self.labels[item], l\n\n    def __len__(self):\n        return len(self.labels)\n    \n    \n# Create a pytorch dataset for beat synced mel spectrograms\nif __name__ == \"__main__\":\n    ##################################################################################\n    # load beat synced mel spectrograms\n    ##################################################################################\n    # Dataset\n    beat_mel_specs = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/', train=True,\n                                         class_mapping=class_mapping, max_length=-1,\n                                         read_spec_fn=read_mel_spectrogram)\n    # Train and Test loaders\n    train_loader_beat_mel, val_loader_beat_mel = torch_train_val_split(beat_mel_specs, 32, 32, val_size=.33)\n    test_dataset_beat_mel = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/', train=False,\n                                                 class_mapping=class_mapping, max_length=-1,\n                                                 read_spec_fn=read_mel_spectrogram)\n    test_loader_beat_mel = DataLoader(test_dataset_beat_mel, batch_size=1)\n    \n    ##################################################################################\n    # load beat synced chroma chromagrams\n    ##################################################################################\n    beat_chroma = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/', train=True,\n                                     class_mapping=class_mapping, max_length=-1,\n                                     read_spec_fn=read_chromagram)\n    train_loader_beat_chroma, val_loader_beat_chroma = torch_train_val_split(beat_chroma, 32, 32, val_size=.33)\n    test_dataset_beat_chroma = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/', train=False,\n                                                 class_mapping=class_mapping, max_length=-1,\n                                                 read_spec_fn=read_chromagram)\n    test_loader_beat_chroma = DataLoader(test_dataset_beat_chroma, batch_size=1)\n\n    ##################################################################################\n    # load fused speectrogram + chromagram for the full (non-beat-synced) data\n    ##################################################################################\n    specs_fused = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/', train=True,\n                                     class_mapping=class_mapping, max_length=-1,\n                                     read_spec_fn=read_fused_spectrogram)\n    train_loader, val_loader = torch_train_val_split(specs_fused, 32, 32, val_size=.33)\n    test_dataset = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/', train=False,\n                                     class_mapping=class_mapping, max_length=-1,\n                                     read_spec_fn=read_fused_spectrogram)\n    test_loader = DataLoader(test_dataset, batch_size=1)\n    \n    ##################################################################################\n    # load single synced mel spectrograms\n    ##################################################################################\n    # Dataset\n    mel_specs = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/', train=True,\n                                        class_mapping=class_mapping, max_length=-1,\n                                        read_spec_fn=read_mel_spectrogram)\n    # Train and Test loaders\n    train_loader_mel, val_loader_mel = torch_train_val_split(mel_specs, 32, 32, val_size=.33)\n    test_dataset_mel = SpectrogramDataset('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/', train=False,\n                                            class_mapping=class_mapping, max_length=-1,\n                                            read_spec_fn=read_mel_spectrogram)\n    \n    test_loader_mel = DataLoader(test_dataset_mel, batch_size=1)\n\n# %% [markdown]\n# Το όρισμα dataset μας επιτρέπει να ορίσουμε εμείς το μέγεθος της ακολουθίας που τελικά θα χρησιμοποιήθει. Με αυτό τον τρόπο μπορούμε να τρέξουμε το νευρωνικό δίκτυο με batches. Στην μελέτη μας, επιλέγουμε να το ορίσουμε ως το μέγιστο μήκος της ακολουθίας.\n# \n# Όταν κάνουμε πειράματα (debugging) με τα μοντέλα, δεν πρέπει να έχουμε καμία τυχαιότητα στην επιλογή των δύο dataset (ισοδύναμα seed=None). Αυτό συμβαίνει, διότι θέλουμε να ελέγχουμε την επίδοση διάφορων υπερπαραμέτρων (batch size, size of layers, etc) και αν επιτρέπουμε τυχαιότητα στο dataset, δεν θα είναι δυνατό να ερμηνεύσουμε βελτίωση στο accuracy.\n\n# %% [markdown]\n# **Lets see an example sample from the dataset**\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:30:34.201329Z\",\"iopub.execute_input\":\"2023-01-08T18:30:34.201629Z\",\"iopub.status.idle\":\"2023-01-08T18:30:34.310187Z\",\"shell.execute_reply.started\":\"2023-01-08T18:30:34.201596Z\",\"shell.execute_reply\":\"2023-01-08T18:30:34.309428Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndatum = next(iter(train_loader_beat_mel))\nprint('Data shape')\nprint(datum[0].shape)  # shape of data\nprint('Labels')\nprint(datum[1])  # labels in batch\nprint('Lengths')\nprint(datum[2])  # length of each element in batch\n\n# %% [markdown]\n# **Lets see an example sample from the beat chroma dataset**\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:30:42.700701Z\",\"iopub.execute_input\":\"2023-01-08T18:30:42.701024Z\",\"iopub.status.idle\":\"2023-01-08T18:30:42.712016Z\",\"shell.execute_reply.started\":\"2023-01-08T18:30:42.700992Z\",\"shell.execute_reply\":\"2023-01-08T18:30:42.711135Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndatum = next(iter(train_loader_beat_chroma))\nprint('Data shape')\nprint(datum[0].shape)  # shape of data\nprint('Labels')\nprint(datum[1])  # labels in batch\nprint('Lengths')\nprint(datum[2])  # length of each element in batch\n\n# %% [markdown]\n# #### Create a pytorch dataset for the fused spectrogram + chromagram for the full (non-beat-synced) data\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:30:49.424574Z\",\"iopub.execute_input\":\"2023-01-08T18:30:49.425246Z\",\"iopub.status.idle\":\"2023-01-08T18:30:49.504269Z\",\"shell.execute_reply.started\":\"2023-01-08T18:30:49.425204Z\",\"shell.execute_reply\":\"2023-01-08T18:30:49.503480Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndatum = next(iter(train_loader))\nprint('Data shape')\nprint(datum[0].shape)  # shape of data\nprint('Labels')\nprint(datum[1])  # labels in batch\nprint('Lengths')\nprint(datum[2])  # length of each element in batch\n\n# %% [markdown]\n# # Ιστογράμματα\n\n# %% [markdown]\n# ## Before mapping\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:30:57.271657Z\",\"iopub.execute_input\":\"2023-01-08T18:30:57.272402Z\",\"iopub.status.idle\":\"2023-01-08T18:30:57.618994Z\",\"shell.execute_reply.started\":\"2023-01-08T18:30:57.272364Z\",\"shell.execute_reply\":\"2023-01-08T18:30:57.618176Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ny_train = []\ny_test = []\n\n# Train labels\nfile1 = open('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/train_labels.txt', 'r') \nLines = file1.readlines()[1:] \nfor line in Lines: \n    label = line.split()[1]\n    y_train.append(label)\n    \n# Test labels\nfile1 = open('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/test_labels.txt', 'r') \nLines = file1.readlines()[1:] \nfor line in Lines: \n    label = line.split()[1]\n    y_test.append(label)\n\n# before mapping\nbins = np.arange(len(np.unique(y_train))+1) - 0.5\n# train\nplt.figure(figsize=(24,8))\nplt.title('y_train', fontsize=16)\nn, bins, patches = plt.hist(y_train, bins, facecolor='blue', rwidth=0.25)\nplt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:31:04.425715Z\",\"iopub.execute_input\":\"2023-01-08T18:31:04.426442Z\",\"iopub.status.idle\":\"2023-01-08T18:31:04.734757Z\",\"shell.execute_reply.started\":\"2023-01-08T18:31:04.426407Z\",\"shell.execute_reply\":\"2023-01-08T18:31:04.734063Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# before mapping\nbins = np.arange(len(np.unique(y_test))+1) - 0.5\n# test\nplt.figure(figsize=(24,8))\nplt.title('y_test', fontsize=16)\nn, bins, patches = plt.hist(y_test, bins, facecolor='blue', rwidth=0.25)\nplt.show()\n\n# %% [markdown]\n# ## After mapping\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:31:11.216156Z\",\"iopub.execute_input\":\"2023-01-08T18:31:11.216438Z\",\"iopub.status.idle\":\"2023-01-08T18:31:11.485288Z\",\"shell.execute_reply.started\":\"2023-01-08T18:31:11.216408Z\",\"shell.execute_reply\":\"2023-01-08T18:31:11.484625Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ny_train = []\ny_test = []\n\n# Train labels\nfile1 = open('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/train_labels.txt', 'r') \nLines = file1.readlines()[1:] \nfor line in Lines: \n    label = line.split()[1]\n    if class_mapping[label] is not None:\n        y_train.append(class_mapping[label])\n    \n# Test labels\nfile1 = open('../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/test_labels.txt', 'r') \nLines = file1.readlines()[1:] \nfor line in Lines: \n    label = line.split()[1]\n    if class_mapping[label] is not None:\n        y_test.append(class_mapping[label])\n        \n# after mapping\nbins = np.arange(len(np.unique(y_train))+1) - 0.5\n# train\nplt.figure(figsize=(24,8))\nplt.title('y_train', fontsize=16)\nn, bins, patches = plt.hist(y_train, bins, facecolor='red', rwidth=0.75)\nplt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-01-08T18:31:18.460509Z\",\"iopub.execute_input\":\"2023-01-08T18:31:18.460792Z\",\"iopub.status.idle\":\"2023-01-08T18:31:18.702954Z\",\"shell.execute_reply.started\":\"2023-01-08T18:31:18.460747Z\",\"shell.execute_reply\":\"2023-01-08T18:31:18.702264Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# after mapping\nbins = np.arange(len(np.unique(y_test))+1) - 0.5\n# test\nplt.figure(figsize=(24,8))\nplt.title('y_test', fontsize=16)\nn, bins, patches = plt.hist(y_test, bins, facecolor='red', rwidth=0.75)\nplt.show()","metadata":{"_uuid":"375c6dbe-5d57-4bef-a206-29bad91d81bb","_cell_guid":"fd9105c6-c4a5-4aac-a371-786b007aa801","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}